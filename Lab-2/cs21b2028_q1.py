# -*- coding: utf-8 -*-
"""cs21b2028_q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13cCw_N00naqgIQoDL14UoWOgavyv52Cr
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

from google.colab import drive
drive.mount('/content/drive')

path='/content/drive/MyDrive/Colab Notebooks/CS21B2028/PR ML Lab/datasets/'
df=pd.read_csv(path+'iris.csv')

#question a(i)

#extracting features
x=df[['SepalLengthCm', 'SepalWidthCm']].values
y=df['Species'].values

#splitting data into training and testing data sets
def split_data(x,y,num_samples_per_class=3):
  trainindices=[]
  testindices=[]

  for class_label in np.unique(y):
        class_indices = np.where(y == class_label)[0]
        trainindices.extend(class_indices[num_samples_per_class:])
        testindices.extend(class_indices[:num_samples_per_class])

  return x[trainindices], y[trainindices], x[testindices], y[testindices]

x_train, y_train, x_test, y_test = split_data(x, y, num_samples_per_class=3)

#question a(ii)

# Plotting the training samples using scatter plot
plt.figure(figsize=(10, 6))
for class_label in np.unique(y_train):
    class_indices = np.where(y_train == class_label)[0]
    plt.scatter(x_train[class_indices, 0], x_train[class_indices, 1], label=f'Class {class_label}')
plt.xlabel('Sepal Length')
plt.ylabel('Sepal Width')
plt.title('Training Samples')
plt.legend()
plt.show()

#question a(iii)

# Function to calculate Euclidean distance
def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2)**2))

# Compute Euclidean distances for each test sample to all training samples
distances = []
for test_sample in x_test:
    sample_distances = [euclidean_distance(test_sample, train_sample) for train_sample in x_train]
    distances.append(sample_distances)

distances = np.array(distances)

print("Euclidean Distances:")
print(distances)

#question a(iv)

# Identify K-nearest neighbors for each test sample
k = 3  # Set your desired value of K
k_nearest_neighbors = []
for test_sample in x_test:
    sample_distances = [euclidean_distance(test_sample, train_sample) for train_sample in x_train]
    nearest_indices = np.argsort(sample_distances)[:k]
    k_nearest_neighbors.append(nearest_indices)

k_nearest_neighbors = np.array(k_nearest_neighbors)

print(f"Indices of K-nearest neighbors for each test sample (K = {k}):")
print(k_nearest_neighbors)

#question a(v)

# Determine majority class among K-nearest neighbors
predicted_classes = []

for neighbor_indices in k_nearest_neighbors:
    neighbor_labels = y_train[neighbor_indices]
    unique_labels, label_counts = np.unique(neighbor_labels, return_counts=True)
    predicted_class = unique_labels[np.argmax(label_counts)]
    predicted_classes.append(predicted_class)

predicted_classes = np.array(predicted_classes)

print(f"Predicted classes for each test sample (K = {k}):")
print(predicted_classes)

#question a(vi)

# Evaluate accuracy
accuracy = np.mean(predicted_classes == y_test)
print(f"Accuracy for K = {k}: {accuracy:.2f}")

#QUESTION B

x = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values
y = df['Species'].values


def split_data(x,y,num_samples_per_class=3):
  trainindices=[]
  testindices=[]

  for class_label in np.unique(y):
        class_indices = np.where(y == class_label)[0]
        trainindices.extend(class_indices[num_samples_per_class:])
        testindices.extend(class_indices[:num_samples_per_class])

  return x[trainindices], y[trainindices], x[testindices], y[testindices]

x_train, y_train, x_test, y_test = split_data(x, y, num_samples_per_class=3)



# Function to calculate Euclidean distance
def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2)**2))

# Compute Euclidean distances for each test sample to all training samples
distances = []
for test_sample in x_test:
    sample_distances = [euclidean_distance(test_sample, train_sample) for train_sample in x_train]
    distances.append(sample_distances)

distances = np.array(distances)

print("Euclidean Distances:")
print(distances)



# K-nearest neighbors classification
def knn_classify(X_train, y_train, X_test, k):
    predicted_classes = []

    for test_sample in X_test:
        sample_distances = [euclidean_distance(test_sample, train_sample) for train_sample in X_train]
        nearest_indices = np.argsort(sample_distances)[:k]
        neighbor_labels = y_train[nearest_indices]
        unique_labels, label_counts = np.unique(neighbor_labels, return_counts=True)
        predicted_class = unique_labels[np.argmax(label_counts)]
        predicted_classes.append(predicted_class)

    return predicted_classes


# Identify K-nearest neighbors for each test sample
k = 3  # Set your desired value of K
k_nearest_neighbors = []
for test_sample in x_test:
    sample_distances = [euclidean_distance(test_sample, train_sample) for train_sample in x_train]
    nearest_indices = np.argsort(sample_distances)[:k]
    k_nearest_neighbors.append(nearest_indices)

k_nearest_neighbors = np.array(k_nearest_neighbors)

print(f"Indices of K-nearest neighbors for each test sample (K = {k}):")
print(k_nearest_neighbors)


# Determine majority class among K-nearest neighbors
predicted_classes = []

for neighbor_indices in k_nearest_neighbors:
    neighbor_labels = y_train[neighbor_indices]
    unique_labels, label_counts = np.unique(neighbor_labels, return_counts=True)
    predicted_class = unique_labels[np.argmax(label_counts)]
    predicted_classes.append(predicted_class)

predicted_classes = np.array(predicted_classes)

print(f"Predicted classes for each test sample (K = {k}):")
print(predicted_classes)


# Evaluate accuracy
def evaluate_accuracy(predicted_classes, y_test):
    accuracy = np.mean(predicted_classes == y_test)
    return accuracy

# Experiment with different values of K
for k in range(1, 11):  # Try K values from 1 to 10
    predicted_classes = knn_classify(x_train, y_train, x_test, k)
    accuracy = evaluate_accuracy(predicted_classes, y_test)
    print(f"Accuracy for K = {k}: {accuracy:.2f}")